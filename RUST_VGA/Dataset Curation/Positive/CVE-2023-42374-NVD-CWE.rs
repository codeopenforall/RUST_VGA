fn create_p2p_network(
        config: &NodeConfig,
        state_sync_store: RocksDbStore,
        chain_identifier: ChainIdentifier,
        trusted_peer_change_rx: watch::Receiver<TrustedPeerChangeEvent>,
        archive_readers: ArchiveReaderBalancer,
        prometheus_registry: &Registry,
    ) -> Result<(Network, discovery::Handle, state_sync::Handle)> {
        let (state_sync, state_sync_server) = state_sync::Builder::new()
            .config(config.p2p_config.state_sync.clone().unwrap_or_default())
            .store(state_sync_store)
            .archive_readers(archive_readers)
            .with_metrics(prometheus_registry)
            .build();

        let (discovery, discovery_server) = discovery::Builder::new(trusted_peer_change_rx)
            .config(config.p2p_config.clone())
            .build();

        let p2p_network = {
            let routes = anemo::Router::new()
                .add_rpc_service(discovery_server)
                .add_rpc_service(state_sync_server);

            let inbound_network_metrics =
                NetworkMetrics::new("sui", "inbound", prometheus_registry);
            let outbound_network_metrics =
                NetworkMetrics::new("sui", "outbound", prometheus_registry);

            let service = ServiceBuilder::new()
                .layer(
                    TraceLayer::new_for_server_errors()
                        .make_span_with(DefaultMakeSpan::new().level(tracing::Level::INFO))
                        .on_failure(DefaultOnFailure::new().level(tracing::Level::WARN)),
                )
                .layer(CallbackLayer::new(MetricsMakeCallbackHandler::new(
                    Arc::new(inbound_network_metrics),
                    config.p2p_config.excessive_message_size(),
                )))
                .service(routes);

            let outbound_layer = ServiceBuilder::new()
                .layer(
                    TraceLayer::new_for_client_and_server_errors()
                        .make_span_with(DefaultMakeSpan::new().level(tracing::Level::INFO))
                        .on_failure(DefaultOnFailure::new().level(tracing::Level::WARN)),
                )
                .layer(CallbackLayer::new(MetricsMakeCallbackHandler::new(
                    Arc::new(outbound_network_metrics),
                    config.p2p_config.excessive_message_size(),
                )))
                .into_inner();

            let mut anemo_config = config.p2p_config.anemo_config.clone().unwrap_or_default();
            // Set the max_frame_size to be 2 GB to work around the issue of there being too many
            // staking events in the epoch change txn.
            anemo_config.max_frame_size = Some(2 << 30);

            // Set a higher default value for socket send/receive buffers if not already
            // configured.
            let mut quic_config = anemo_config.quic.unwrap_or_default();
            if quic_config.socket_send_buffer_size.is_none() {
                quic_config.socket_send_buffer_size = Some(20 << 20);
            }
            if quic_config.socket_receive_buffer_size.is_none() {
                quic_config.socket_receive_buffer_size = Some(20 << 20);
            }
            quic_config.allow_failed_socket_buffer_size_setting = true;
            anemo_config.quic = Some(quic_config);

            let server_name = format!("sui-{}", chain_identifier);
            let network = Network::bind(config.p2p_config.listen_address)
                .server_name(&server_name)
                .private_key(config.network_key_pair().copy().private().0.to_bytes())
                .config(anemo_config)
                .outbound_request_layer(outbound_layer)
                .start(service)?;
            info!(
                server_name = server_name,
                "P2p network started on {}",
                network.local_addr()
            );

            network
        };

        let discovery_handle = discovery.start(p2p_network.clone());
        let state_sync_handle = state_sync.start(p2p_network.clone());

        Ok((p2p_network, discovery_handle, state_sync_handle))
    }

fn decode(&mut self, buf: &mut DecodeBuf<'_>) -> Result<Option<Self::Item>, Self::Error> {
        if !buf.has_remaining() {
            return Ok(None);
        }

        let chunk = buf.chunk();

        let item: Self::Item =
            bcs::from_bytes(chunk).map_err(|e| Status::internal(e.to_string()))?;
        buf.advance(chunk.len());

        Ok(Some(item))
    }

pub fn spawn(
        authority: Authority,
        keypair: NetworkKeyPair,
        id: WorkerId,
        committee: Committee,
        worker_cache: WorkerCache,
        protocol_config: ProtocolConfig,
        parameters: Parameters,
        validator: impl TransactionValidator,
        client: NetworkClient,
        store: DBMap<BatchDigest, Batch>,
        metrics: Metrics,
        tx_shutdown: &mut PreSubscribedBroadcastSender,
    ) -> Vec<JoinHandle<()>> {
        let worker_name = keypair.public().clone();
        let worker_peer_id = PeerId(worker_name.0.to_bytes());
        info!("Boot worker node with id {} peer id {}", id, worker_peer_id,);

        // Define a worker instance.
        let worker = Self {
            authority: authority.clone(),
            keypair,
            id,
            committee: committee.clone(),
            worker_cache: worker_cache.clone(),
            protocol_config: protocol_config.clone(),
            parameters: parameters.clone(),
            store,
        };

        let node_metrics = Arc::new(metrics.worker_metrics.unwrap());
        let endpoint_metrics = metrics.endpoint_metrics.unwrap();
        let channel_metrics: Arc<WorkerChannelMetrics> = Arc::new(metrics.channel_metrics.unwrap());
        let inbound_network_metrics = Arc::new(metrics.inbound_network_metrics.unwrap());
        let outbound_network_metrics = Arc::new(metrics.outbound_network_metrics.unwrap());
        let network_connection_metrics = metrics.network_connection_metrics.unwrap();

        let mut shutdown_receivers = tx_shutdown.subscribe_n(NUM_SHUTDOWN_RECEIVERS);

        let mut worker_service = WorkerToWorkerServer::new(WorkerReceiverHandler {
            protocol_config: protocol_config.clone(),
            id: worker.id,
            client: client.clone(),
            store: worker.store.clone(),
            validator: validator.clone(),
        });
        // Apply rate limits from configuration as needed.
        if let Some(limit) = parameters.anemo.report_batch_rate_limit {
            worker_service = worker_service.add_layer_for_report_batch(InboundRequestLayer::new(
                rate_limit::RateLimitLayer::new(
                    governor::Quota::per_second(limit),
                    rate_limit::WaitMode::Block,
                ),
            ));
        }
        if let Some(limit) = parameters.anemo.request_batches_rate_limit {
            worker_service = worker_service.add_layer_for_request_batches(
                InboundRequestLayer::new(rate_limit::RateLimitLayer::new(
                    governor::Quota::per_second(limit),
                    rate_limit::WaitMode::Block,
                )),
            );
        }

        // Legacy RPC interface, only used by delete_batches() for external consensus.
        let primary_service = PrimaryToWorkerServer::new(PrimaryReceiverHandler {
            authority_id: worker.authority.id(),
            id: worker.id,
            committee: worker.committee.clone(),
            protocol_config: protocol_config.clone(),
            worker_cache: worker.worker_cache.clone(),
            store: worker.store.clone(),
            request_batches_timeout: worker.parameters.sync_retry_delay,
            request_batches_retry_nodes: worker.parameters.sync_retry_nodes,
            network: None,
            batch_fetcher: None,
            validator: validator.clone(),
        });

        // Receive incoming messages from other workers.
        let address = worker
            .worker_cache
            .worker(authority.protocol_key(), &id)
            .expect("Our public key or worker id is not in the worker cache")
            .worker_address;
        let address = address
            .replace(0, |_protocol| Some(Protocol::Ip4(Ipv4Addr::UNSPECIFIED)))
            .unwrap();
        let addr = address.to_anemo_address().unwrap();

        let epoch_string: String = committee.epoch().to_string();

        // Set up anemo Network.
        let our_primary_peer_id = PeerId(authority.network_key().0.to_bytes());
        let primary_to_worker_router = anemo::Router::new()
            .add_rpc_service(primary_service)
            // Add an Authorization Layer to ensure that we only service requests from our primary
            .route_layer(RequireAuthorizationLayer::new(AllowedPeers::new([
                our_primary_peer_id,
            ])))
            .route_layer(RequireAuthorizationLayer::new(AllowedEpoch::new(
                epoch_string.clone(),
            )));

        let worker_peer_ids = worker_cache
            .all_workers()
            .into_iter()
            .map(|(worker_name, _)| PeerId(worker_name.0.to_bytes()));
        let routes = anemo::Router::new()
            .add_rpc_service(worker_service)
            .route_layer(RequireAuthorizationLayer::new(AllowedPeers::new(
                worker_peer_ids,
            )))
            .route_layer(RequireAuthorizationLayer::new(AllowedEpoch::new(
                epoch_string.clone(),
            )))
            .merge(primary_to_worker_router);

        let service = ServiceBuilder::new()
            .layer(
                TraceLayer::new_for_server_errors()
                    .make_span_with(DefaultMakeSpan::new().level(tracing::Level::INFO))
                    .on_failure(DefaultOnFailure::new().level(tracing::Level::WARN)),
            )
            .layer(CallbackLayer::new(MetricsMakeCallbackHandler::new(
                inbound_network_metrics,
                parameters.anemo.excessive_message_size(),
            )))
            .layer(CallbackLayer::new(FailpointsMakeCallbackHandler::new()))
            .layer(SetResponseHeaderLayer::overriding(
                EPOCH_HEADER_KEY.parse().unwrap(),
                epoch_string.clone(),
            ))
            .service(routes);

        let outbound_layer = ServiceBuilder::new()
            .layer(
                TraceLayer::new_for_client_and_server_errors()
                    .make_span_with(DefaultMakeSpan::new().level(tracing::Level::INFO))
                    .on_failure(DefaultOnFailure::new().level(tracing::Level::WARN)),
            )
            .layer(CallbackLayer::new(MetricsMakeCallbackHandler::new(
                outbound_network_metrics,
                parameters.anemo.excessive_message_size(),
            )))
            .layer(CallbackLayer::new(FailpointsMakeCallbackHandler::new()))
            .layer(SetRequestHeaderLayer::overriding(
                EPOCH_HEADER_KEY.parse().unwrap(),
                epoch_string,
            ))
            .into_inner();

        let anemo_config = {
            let mut quic_config = anemo::QuicConfig::default();
            // Allow more concurrent streams for burst activity.
            quic_config.max_concurrent_bidi_streams = Some(10_000);
            // Increase send and receive buffer sizes on the worker, since the worker is
            // responsible for broadcasting and fetching payloads.
            // With 200MiB buffer size and ~500ms RTT, the max throughput ~400MiB.
            quic_config.stream_receive_window = Some(100 << 20);
            quic_config.receive_window = Some(200 << 20);
            quic_config.send_window = Some(200 << 20);
            quic_config.crypto_buffer_size = Some(1 << 20);
            quic_config.socket_receive_buffer_size = Some(20 << 20);
            quic_config.socket_send_buffer_size = Some(20 << 20);
            quic_config.allow_failed_socket_buffer_size_setting = true;
            quic_config.max_idle_timeout_ms = Some(30_000);
            // Enable keep alives every 5s
            quic_config.keep_alive_interval_ms = Some(5_000);
            let mut config = anemo::Config::default();
            config.quic = Some(quic_config);
            // Set the max_frame_size to be 2 GB to work around the issue of there being too many
            // delegation events in the epoch change txn.
            config.max_frame_size = Some(2 << 30);
            // Set a default timeout of 300s for all RPC requests
            config.inbound_request_timeout_ms = Some(300_000);
            config.outbound_request_timeout_ms = Some(300_000);
            config.shutdown_idle_timeout_ms = Some(1_000);
            config.connectivity_check_interval_ms = Some(2_000);
            config.connection_backoff_ms = Some(1_000);
            config.max_connection_backoff_ms = Some(20_000);
            config
        };

        let network;
        let mut retries_left = 90;

        loop {
            let network_result = anemo::Network::bind(addr.clone())
                .server_name("narwhal")
                .private_key(worker.keypair.copy().private().0.to_bytes())
                .config(anemo_config.clone())
                .outbound_request_layer(outbound_layer.clone())
                .start(service.clone());
            match network_result {
                Ok(n) => {
                    network = n;
                    break;
                }
                Err(_) => {
                    retries_left -= 1;

                    if retries_left <= 0 {
                        panic!();
                    }
                    error!(
                        "Address {} should be available for the primary Narwhal service, retrying in one second",
                        addr
                    );
                    sleep(Duration::from_secs(1));
                }
            }
        }

        info!("Worker {} listening to worker messages on {}", id, address);

        let batch_fetcher = BatchFetcher::new(
            worker_name,
            network.clone(),
            worker.store.clone(),
            node_metrics.clone(),
            protocol_config.clone(),
        );
        client.set_primary_to_worker_local_handler(
            worker_peer_id,
            Arc::new(PrimaryReceiverHandler {
                authority_id: worker.authority.id(),
                id: worker.id,
                committee: worker.committee.clone(),
                protocol_config,
                worker_cache: worker.worker_cache.clone(),
                store: worker.store.clone(),
                request_batches_timeout: worker.parameters.sync_retry_delay,
                request_batches_retry_nodes: worker.parameters.sync_retry_nodes,
                network: Some(network.clone()),
                batch_fetcher: Some(batch_fetcher),
                validator: validator.clone(),
            }),
        );

        let mut peer_types = HashMap::new();

        let other_workers = worker
            .worker_cache
            .others_workers_by_id(authority.protocol_key(), &id)
            .into_iter()
            .map(|(_, info)| (info.name, info.worker_address));

        // Add other workers we want to talk with to the known peers set.
        for (public_key, address) in other_workers {
            let (peer_id, address) = Self::add_peer_in_network(&network, public_key, &address);
            peer_types.insert(peer_id, "other_worker".to_string());
            info!(
                "Adding others workers with peer id {} and address {}",
                peer_id, address
            );
        }

        // Connect worker to its corresponding primary.
        let (peer_id, address) = Self::add_peer_in_network(
            &network,
            authority.network_key(),
            &authority.primary_address(),
        );
        peer_types.insert(peer_id, "our_primary".to_string());
        info!(
            "Adding our primary with peer id {} and address {}",
            peer_id, address
        );

        // update the peer_types with the "other_primary". We do not add them in the Network
        // struct, otherwise the networking library will try to connect to it
        let other_primaries: Vec<(AuthorityIdentifier, Multiaddr, NetworkPublicKey)> =
            committee.others_primaries_by_id(authority.id());
        for (_, _, network_key) in other_primaries {
            peer_types.insert(
                PeerId(network_key.0.to_bytes()),
                "other_primary".to_string(),
            );
        }

        let (connection_monitor_handle, _) = network::connectivity::ConnectionMonitor::spawn(
            network.downgrade(),
            network_connection_metrics,
            peer_types,
            Some(shutdown_receivers.pop().unwrap()),
        );

        let network_admin_server_base_port = parameters
            .network_admin_server
            .worker_network_admin_server_base_port
            .checked_add(id as u16)
            .unwrap();
        info!(
            "Worker {} listening to network admin messages on 127.0.0.1:{}",
            id, network_admin_server_base_port
        );

        let admin_handles = network::admin::start_admin_server(
            network_admin_server_base_port,
            network.clone(),
            shutdown_receivers.pop().unwrap(),
        );

        let client_flow_handles = worker.handle_clients_transactions(
            vec![
                shutdown_receivers.pop().unwrap(),
                shutdown_receivers.pop().unwrap(),
                shutdown_receivers.pop().unwrap(),
            ],
            node_metrics,
            channel_metrics,
            endpoint_metrics,
            validator,
            client,
            network.clone(),
        );

        let network_shutdown_handle =
            Self::shutdown_network_listener(shutdown_receivers.pop().unwrap(), network);

        // NOTE: This log entry is used to compute performance.
        info!(
            "Worker {} successfully booted on {}",
            id,
            worker
                .worker_cache
                .worker(authority.protocol_key(), &worker.id)
                .expect("Our public key or worker id is not in the worker cache")
                .transactions
        );

        let mut handles = vec![connection_monitor_handle, network_shutdown_handle];
        handles.extend(admin_handles);
        handles.extend(client_flow_handles);
        handles
    }

